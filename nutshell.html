<html><head><link rel="stylesheet" href="slurm-guide-styles.css"></head><body>
    <h1 id="guide-to-using-slurm"><a href="../index.html">Slurm User Guide </a></h1>

    <h3> Understanding Slurm Concepts</h3>

    Before diving into Slurm usage, it's important to understand some key concepts:
    
    <li> Node     : A computer in the cluster. </li>
    <li> Partition: A group of nodes with specific characteristics. </li>
    <li> Job      : A resource allocation request for a specific program or task. </li>
    <li> Task     : An instance of a running program within a job. </li>

    <h3> Access the cluster </h3>

To access the cluster you need a terminal or a terminal emulator.
<pre class="command-line">ssh kuser@supekhead01fl</pre>

where kuser is  your alphanumerical KU user id and supekhead01fl is the head node of the cluster.


<h3> Basic Slurm Commands</h3>

Here are some essential Slurm commands you'll use frequently:
<br>

<li> srun: Run an interactive  job</li>
<li> salloc: Allocate resource for interactive use</li>
<li> sbatch : Submit a batch script </li>
<li> scancel: Cancel a job </li>
<li> scontrol show job job_id: show info about job job_id</li>
<li> squeue : View information about jobs in the queue </li>
<li> sinfo  : View information about Slurm nodes and partitions </li>

<h3>Loading Modules</h3>
<br>
<li> module avail: List all available modules </li>
<li> module list: Show currently loaded modules</li>
<li> module load module_name: Load a specific module</li>
<li> module unload module_name: Unload a specific module</li>
<li> module purge: Unload all currently loaded modules</li>
<li> module show module_name: Display information about a module</li>

<h3> Sbatch scripts</h3>

    <pre class="code">
        #!/bin/bash
        #SBATCH --job-name=my_job        # Name of the job 
        #SBATCH --output=output_%j.log   # Standard output log file (%j is replaced by the job ID)
        #SBATCH --error=error_%j.log     # Standard error log file
        #SBATCH --time=01:00:00          # Time limit (HH:MM:SS)
        #SBATCH --ntasks=1               # Number of tasks (processes)
        #SBATCH --cpus-per-task=1        # Number of CPU cores per task
        #SBATCH --mem=1G                 # Memory limit
        #SBATCH --partition=general      # Partition (queue) name
        #SBATCH --gres=gpu:2             # Request 2 GPUs
        
        Your commands here
        </pre>
        

<h3>  Using Conda in Slurm Jobs </h3>


<p>First, you need to ensure Conda is available in your Slurm job. 
Once Conda is loaded, you can activate your environment. Here's how you might do this in a Slurm script:
</p>


<p >
    <pre class="code">

#!/bin/bash 
#SBATCH  [...]

# Load Conda
module load anaconda3

# Activate your environment
conda activate myenv

# Run your Python script
python my_script.py
    </pre>
</p>




</body>
</html>