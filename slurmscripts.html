<a href="../index.html"></a><html><head><link rel="stylesheet" href="slurm-guide-styles.css"></head><body><h1 id="guide-to-using-slurm"> <a href="../index.html">Slurm User Guide </a></h1>


<h2>Writing Slurm Job Scripts</h2>

A Slurm job script is a shell script (typically bash) that contains both Slurm directives and the commands you want to run on the cluster. 
Let's break down the components and syntax of a Slurm job script:

<h3>  Basic Structure </h3>

<pre class="code">

#!/bin/bash
#SBATCH [options]
#SBATCH [more options]

# Your commands here
</pre>

<h3>  Shebang </h3>

The first line of your script should be the shebang:

<pre class="code">
#!/bin/bash
</pre>

This tells the system to interpret the script using the bash shell.

<h3>  Slurm Directives </h3>

Slurm directives are special comments that start with `#SBATCH`. They tell Slurm how to set up and run your job. Here are some common directives:

<pre class="code">
#SBATCH --job-name=my_job        # Name of the job 
#SBATCH --output=output_%j.log   # Standard output log file (%j is replaced by the job ID)
#SBATCH --error=error_%j.log     # Standard error log file
#SBATCH --time=01:00:00          # Time limit (HH:MM:SS)
#SBATCH --ntasks=1               # Number of tasks (processes)
#SBATCH --cpus-per-task=1        # Number of CPU cores per task
#SBATCH --mem=1G                 # Memory limit
#SBATCH --partition=general      # Partition (queue) name
#SBATCH --gres=gpu:2             # Request 2 GPUs

</pre>

<h3>  Common Slurm Directives </h3>

Here's a more comprehensive list of Slurm directives:

<li> `--job-name=<name>`: Set a name for the job </li>
 <li>`--output=<filename>`: Specify the file for standard output</li>
 <li>`--error=<filename>`: Specify the file for standard error</li>
 <li>`--time=<time>`: Set a time limit for the job (format: DD-HH:MM:SS)</li>
 <li>`--ntasks=<number>`: Specify the number of tasks to run</li>
 <li>`--cpus-per-task=<number>`: Set the number of CPU cores per task</li>
 <li>`--mem=<size[units]>`: Set the total memory required (e.g., 1G for 1 gigabyte)</li>
 <li>`--partition=<partition_name>`: Specify the partition to run the job on</li>
 <li>`--array=<indexes>`: Create a job array (e.g., --array=1-10 for 10 array jobs)</li>
 <li>`--mail-type=<type>`: Specify email notification events (e.g., BEGIN, END, FAIL)</li>
 <li>`--mail-user=<email_address>`: Set the email address for notifications</li>
 <li>`--nodes=<number>`: Request a specific number of nodes</li>
 <li>`--gres=<resource>`: Request generic consumable resources (e.g., GPUs)</li>

<h3>  Environment Variables </h3>

Slurm sets several environment variables that you can use in your script:

<li>- `$SLURM_JOB_ID`: The ID of the job</li>
<li>- `$SLURM_ARRAY_TASK_ID`: The array index for job arrays</li>
<li>- `$SLURM_CPUS_PER_TASK`: Number of CPUs allocated per task</li>
<li>- `$SLURM_NTASKS`: Total number of tasks in a job</li>

<h3>  Example Job Script </h3>

Here's an example of a more complex Slurm job script:

<pre class="code">
    #!/bin/bash 
#SBATCH --job-name=complex_job
#SBATCH --output=output_%A_%a.log
#SBATCH --error=error_%A_%a.log
#SBATCH --array=1-5
#SBATCH --time=02:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=8G
#SBATCH --partition=general
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=your.email@example.com

# Load any necessary modules
module load python/3.8


# Run the main command
python my_script.py --input-file input.txt --output-file output.txt

# Optional: Run some post-processing
if [ $? -eq 0 ]; then
    echo "Job completed successfully"
    python post_process.py output.txt
else
    echo "Job failed"
fi

</pre>

<h3>Understanding --ntasks in Slurm</h3>

    <p>When you use the `--ntasks` option in Slurm without other specifications, it's important to understand how Slurm interprets and applies this setting. 

    <p>When you specify `--ntasks=4` without other options:</p>

    <ul>
        <li>Slurm will allocate resources for 4 tasks.</li>
        <li>By default, each task is allocated 1 CPU (core).</li>
        <li>The tasks may be distributed across multiple nodes, depending on the cluster's configuration and available resources.</li>
    </ul>

    <pre class="code">
#SBATCH --ntasks=4

# This will run your command with 4 tasks
srun ./my_program
    </pre>

    <p>In this scenario:</p>
    <ul>
        <li>Your job will be allocated 4 CPUs in total.</li>
        <li>These 4 CPUs could be on a single node or spread across multiple nodes, depending on availability and the cluster's configuration.</li>
        <li>Each task will have access to 1 CPU by default.</li>
    </ul>

    <h2>Important Considerations</h2>

    <ol>
        <li><strong>CPU Allocation:</strong> Without specifying `--cpus-per-task`, each task gets 1 CPU by default.</li>
        <li><strong>Memory Allocation:</strong> The default memory allocation per task depends on the cluster's configuration. It's often a good practice to specify memory requirements explicitly.</li>
        <li><strong>Node Distribution:</strong> Tasks may be distributed across nodes unless you specify `--nodes` or use the `--ntasks-per-node` option.</li>
        <li><strong>Parallel Execution:</strong> This setting is particularly useful for MPI jobs where you want to run multiple parallel processes.</li>
    </ol>

    <h2>Examples with Additional Specifications</h2>

    <h3>1. Specifying CPUs per Task</h3>
    <pre class="code">
#SBATCH --ntasks=4
#SBATCH --cpus-per-task=2

srun ./my_multi_threaded_program
    </pre>
    <p>This allocates 4 tasks, each with 2 CPUs, totaling 8 CPUs for the job.</p>

    <h3>2. Constraining to a Single Node</h3>
    <pre class="code">
#SBATCH --ntasks=4
#SBATCH --nodes=1

srun ./my_program
    </pre>
    <p>This ensures all 4 tasks run on the same node.</p>

    <h3>3. Specifying Tasks per Node</h3>
    <pre class="code">
#SBATCH --ntasks=4
#SBATCH --ntasks-per-node=2

srun ./my_program
    </pre>
    <p>This distributes the 4 tasks across 2 nodes, with 2 tasks per node.</p>

    

    <h2>Best Practices</h2>
    <ul>
        <li>Be explicit about your resource requirements when possible (CPUs, memory, etc.).</li>
        <li>Consider the nature of your program (MPI, multi-threaded, etc.) when deciding how to allocate tasks and CPUs.</li>
        <li>Use `--ntasks` in combination with other options like `--cpus-per-task` or `--nodes` for more precise control over resource allocation.</li>
        <li>Test your job submissions with smaller task counts before scaling up to ensure proper resource utilization.</li>
<h3>  Best Practices for Job Scripts </h3>
<ol>
<li>Use variables: For repeated values or for clarity, use shell variables.</li>
<li>Comment your script: Explain complex parts of your script for better maintainability.</li>
<li>Error handling: Include error checking and handling in your script.</li>
<li>Modularity: For complex workflows, consider breaking your job into multiple scripts.</li>
<li>Resource estimation: Start with conservative resource estimates and adjust based on actual usage.</li>
<li>Environment setup: Load necessary modules and set environment variables at the beginning of your script.</li>
<li>Output management: Use job ID and array ID in output file names to avoid overwrites.</li>
</ol>
Remember to write your commands in the LAST part of the script, after you declared all the Slurm directives!
</body>
</html>